{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import itertools\n",
    "from random import sample\n",
    "import numpy as np\n",
    "# Preprocessing\n",
    "data_file = 'train.json'\n",
    "noodles_data = 'noodles.txt'\n",
    "\n",
    "UNK = 'unk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(data_file) as recipe_data:\n",
    "    data = json.load(recipe_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ingredients_list = [item['ingredients'] for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noodles_only = []\n",
    "for ingredients in ingredients_list:\n",
    "    for ingredient in ingredients:\n",
    "        if 'noodles' in ingredient:\n",
    "            noodles_only.append(ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Writing to the file\n",
    "with open(noodles_data, 'w') as nd:\n",
    "    json.dump(noodles_only, nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pork loin', 'roasted peanuts', 'chopped cilantro fresh', 'hoisin sauce', 'creamy peanut butter', 'chopped fresh mint', 'thai basil', 'rice', 'medium shrimp', 'water', 'rice noodles', 'beansprouts']\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into recipe title and ingredients\n",
    "print(noodles_only[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def title_ingredient_split(ingredients):\n",
    "    title = {ingredient for ingredient in ingredients if 'noodles' in ingredient}\n",
    "    _ingredients = list(set(ingredients) - set(title))\n",
    "    return list(title)[0], _ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get all \n",
    "titles = []\n",
    "ingredients = []\n",
    "V = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for noodles in noodles_only:\n",
    "    title, ingredient = title_ingredient_split(noodles)\n",
    "    for ing in ingredient:\n",
    "        for i in ing.split(\" \"):\n",
    "            V.append(i)\n",
    "    for t in title.split(\" \"):\n",
    "        V.append(t)\n",
    "    titles.append(title.lower())\n",
    "    ingredients.append(\" \".join(ing for ing in ingredient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Vocabulary = list(set(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EN_WHITELIST = '0123456789abcdefghijklmnopqrstuvwxyz ' # space is included in whitelist\n",
    "EN_BLACKLIST = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\''\n",
    "\n",
    "def zero_pad(qtokenized, atokenized, w2idx):\n",
    "    # num of rows\n",
    "    data_len = len(qtokenized)\n",
    "    \n",
    "    print(data_len)\n",
    "\n",
    "    # numpy arrays to store indices\n",
    "    idx_q = np.zeros([data_len, limit['maxq']], dtype=np.int32) \n",
    "    idx_a = np.zeros([data_len, limit['maxa']], dtype=np.int32)\n",
    "\n",
    "    for i in range(data_len):\n",
    "        q_indices = pad_seq(qtokenized[i], w2idx, limit['maxq'])\n",
    "        a_indices = pad_seq(atokenized[i], w2idx, limit['maxa'])\n",
    "\n",
    "        idx_q[i] = np.array(q_indices)\n",
    "        idx_a[i] = np.array(a_indices)\n",
    "\n",
    "    return idx_q, idx_a\n",
    "\n",
    "\n",
    "def pad_seq(seq, lookup, maxlen):\n",
    "    indices = []\n",
    "    for word in seq:\n",
    "        if word in lookup:\n",
    "            indices.append(lookup[word])\n",
    "        else:\n",
    "            indices.append(lookup[UNK])\n",
    "    return indices + [0]*(maxlen - len(seq))\n",
    "\n",
    "\n",
    "def index_(tokenized_sentences, vocab_size):\n",
    "    # get frequency distribution\n",
    "    freq_dist = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "    # get vocabulary of 'vocab_size' most used words\n",
    "    vocab = freq_dist.most_common(vocab_size)\n",
    "    # index2word\n",
    "    index2word = ['_'] + [UNK] + [ x[0] for x in vocab ]\n",
    "    # word2index\n",
    "    word2index = dict([(w,i) for i,w in enumerate(index2word)] )\n",
    "    return index2word, word2index, freq_dist\n",
    "\n",
    "\n",
    "def split_dataset(x, y, ratio = [0.7, 0.15, 0.15] ):\n",
    "    # number of examples\n",
    "    data_len = len(x)\n",
    "    lens = [ int(data_len*item) for item in ratio ]\n",
    "\n",
    "    trainX, trainY = x[:lens[0]], y[:lens[0]]\n",
    "    testX, testY = x[lens[0]:lens[0]+lens[1]], y[lens[0]:lens[0]+lens[1]]\n",
    "    validX, validY = x[-lens[-1]:], y[-lens[-1]:]\n",
    "\n",
    "    return (trainX,trainY), (testX,testY), (validX,validY)\n",
    "\n",
    "\n",
    "\n",
    "def batch_gen(x, y, batch_size):\n",
    "    # infinite while\n",
    "    while True:\n",
    "        for i in range(0, len(x), batch_size):\n",
    "            if (i+1)*batch_size < len(x):\n",
    "                yield x[i : (i+1)*batch_size ].T, y[i : (i+1)*batch_size ].T\n",
    "\n",
    "\n",
    "def rand_batch_gen(x, y, batch_size):\n",
    "    while True:\n",
    "        sample_idx = sample(list(np.arange(len(x))), batch_size)\n",
    "        yield x[sample_idx].T, y[sample_idx].T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def decode(sequence, lookup, separator=''): # 0 used for padding, is ignored\n",
    "    return separator.join([ lookup[element] for element in sequence if element ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Begin processing\n",
    "recipe_tokenized = [ wordlist.split(' ') for wordlist in ingredients ]\n",
    "title_tokenized = [ wordlist.split(' ') for wordlist in titles ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1412\n"
     ]
    }
   ],
   "source": [
    "limit = {\n",
    "        'maxq' : 100,\n",
    "        'minq' : 1,\n",
    "        'maxa' : 5,\n",
    "        'mina' : 1\n",
    "        }\n",
    "\n",
    "\n",
    "idx2w, w2idx, freq_dist = index_( recipe_tokenized + title_tokenized, vocab_size=VOCAB_SIZE)\n",
    "idx_q, idx_a = zero_pad(recipe_tokenized, title_tokenized, w2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(trainX, trainY), (testX, testY), (validX, validY) = split_dataset(idx_a, idx_q, ratio=[.8, .1, .1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let us now save the necessary dictionaries\n",
    "metadata = {\n",
    "        'w2idx' : w2idx,\n",
    "        'idx2w' : idx2w,\n",
    "        'limit' : limit,\n",
    "        'freq_dist' : freq_dist\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xseq_len = trainX.shape[-1]\n",
    "yseq_len = trainY.shape[-1]\n",
    "batch_size = 4\n",
    "xvocab_size = len(metadata['idx2w'])  \n",
    "yvocab_size = xvocab_size\n",
    "emb_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seq2seq_wrapper\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'seq2seq_wrapper' from 'E:\\\\rnn\\\\practical_seq2seq\\\\seq2seq_wrapper.py'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(seq2seq_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<log> Building Graph </log>"
     ]
    }
   ],
   "source": [
    "model = seq2seq_wrapper.Seq2Seq(xseq_len=xseq_len,\n",
    "                               yseq_len=yseq_len,\n",
    "                               xvocab_size=xvocab_size,\n",
    "                               yvocab_size=yvocab_size,\n",
    "                               ckpt_path='ckpt/noodles/',\n",
    "                               emb_dim=emb_dim,\n",
    "                               num_layers=3,\n",
    "                                epochs=100000,\n",
    "                                lr=0.0001\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_batch_gen = rand_batch_gen(validX, validY, 141)\n",
    "test_batch_gen = rand_batch_gen(testX, testY, 141)\n",
    "train_batch_gen = rand_batch_gen(trainX, trainY, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sess = model.train(train_batch_gen, val_batch_gen)\n",
    "sess = model.restore_last_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141, 100)\n"
     ]
    }
   ],
   "source": [
    "input_ = test_batch_gen.__next__()[0]\n",
    "output = model.predict(sess, input_)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "replies = []\n",
    "questions = []\n",
    "for ii, oi in zip(input_.T, output):\n",
    "    q = decode(sequence=ii, lookup=metadata['idx2w'], separator=' ')\n",
    "    decoded = decode(sequence=oi, lookup=metadata['idx2w'], separator=' ').split(' ')\n",
    "    if decoded.count('unk') == 0:\n",
    "        if decoded not in replies:\n",
    "#             print('title : [{0}]; recipe : [{1}]'.format(q, (' '.join(decoded))))\n",
    "            questions.append(q)\n",
    "            replies.append(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "replies = [list(set(reply)) for reply in replies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe title: lasagna noodles\n",
      " Ingredients: ['pepper', 'mein', 'paste', 'garlic', 'cheese', 'mozzarella', 'oil', 'onions', 'ricotta', 'cabbage', 'beef', 'sauce', 'eggs', 'rice', 'ground', 'olive']\n",
      "\n",
      "Recipe title: noodles\n",
      " Ingredients: ['pepper', 'carrots', 'stock', 'pork', 'onions', 'sugar', 'dashi', 'eggs', 'sauce', 'rice', 'chicken', 'soy']\n",
      "\n",
      "Recipe title: mung bean noodles\n",
      " Ingredients: ['pepper', 'mushrooms', 'carrots', 'vegetable', 'pork', 'oil', 'sugar', 'sharp', 'rice', 'black', 'ground']\n",
      "\n",
      "Recipe title: lasagna noodles, cooked and drained\n",
      " Ingredients: ['garlic', 'spray', 'cooking', 'rice', 'vinegar', 'eye', 'olive', 'soybean', 'ulek', 'oil', 'onions', 'dried', 'vegetables', 'ground', 'green', 'butter', 'andouille', 'flour', 'boneless', 'cheese', 'cloves', 'whole', 'all-purpose', 'pepper', 'paper', 'star', 'black']\n",
      "\n",
      "Recipe title: dried rice noodles\n",
      " Ingredients: ['paste', 'stock', 'reduced', 'onions', 'ginger', 'hoisin', 'shank', 'fresh', 'fish', 'sauce', 'rice', 'root', 'beansprouts', 'green']\n",
      "\n",
      "Recipe title: egg noodles\n",
      " Ingredients: ['carrots', 'garlic', 'oil', 'onions', 'part-skim', 'sesame', 'cooking', 'sauce', 'rice', 'soy', 'beansprouts', 'green']\n",
      "\n",
      "Recipe title: chuka soba noodles\n",
      " Ingredients: ['carrots', 'oil', 'onions', 'cucumber', 'sesame', 'sauce', 'peeled', 'soy', 'vinegar', 'salt', 'green']\n",
      "\n",
      "Recipe title: ramen noodles\n",
      " Ingredients: ['snow', 'starch', 'garlic', 'spinach', 'oyster', 'rice', 'soy', 'breasts', 'chinese', 'stock', 'sour', 'oil', 'onions', 'tomato', 'brown', 'chicken', 'corn', 'green', 'broccoli', 'baby', 'tofu', 'red', 'ginger', 'sesame', 'yakisoba', 'beansprouts', 'pepper', 'mushrooms', 'sugar', 'wine', 'Sriracha', 'sauce', 'powder', 'bell', 'black', 'salt']\n",
      "\n",
      "Recipe title: rice noodles\n",
      " Ingredients: ['pepper', 'carrots', 'stock', 'pork', 'oil', 'onions', 'sugar', 'dashi', 'sesame', 'sauce', 'rice', 'chicken', 'soy']\n",
      "\n",
      "Recipe title: lo mein noodles\n",
      " Ingredients: ['mushrooms', 'carrots', 'garlic', 'chopped', 'oil', 'sliced', 'ginger', 'seeds', 'sesame', 'sauce', 'soy', 'mirin']\n",
      "\n",
      "Recipe title: vermicelli noodles\n",
      " Ingredients: ['pepper', 'crushed', 'chopped', 'pork', 'onions', 'candlenuts', 'water', 'shrimp', 'rice']\n",
      "\n",
      "Recipe title: chinese egg noodles\n",
      " Ingredients: ['peanut', 'carrots', 'chopped', 'garlic', 'oil', 'ginger', 'minced', 'oyster', 'sesame', 'sauce', 'fresh', 'soy', 'vinegar', 'salt']\n",
      "\n",
      "Recipe title: rice stick noodles\n",
      " Ingredients: ['carrots', 'garlic', 'oil', 'onions', 'sugar', 'sesame', 'sauce', 'chili', 'rice', 'soy', 'green']\n",
      "\n",
      "Recipe title: wide rice noodles\n",
      " Ingredients: ['carrots', 'chopped', 'garlic', 'oil', 'onions', 'sugar', 'shiitake', 'sesame', 'sauce', 'rice', 'soy']\n",
      "\n",
      "Recipe title: glass noodles\n",
      " Ingredients: ['parsley', 'garlic', 'grated', 'eggs', 'rice', 'shredded', 'olive', 'oil', 'onions', 'dried', 'vegetables', 'ground', 'parmesan', 'cheese', 'mozzarella', 'ricotta', 'pepper', 'cabbage', 'sauce', 'powder', 'black', 'oregano']\n",
      "\n",
      "Recipe title: chinese wheat noodles\n",
      " Ingredients: ['carrots', 'oil', 'onions', 'sesame', 'Sriracha', 'sauce', 'chili', 'rice', 'soy', 'vinegar', 'salt', 'green']\n",
      "\n",
      "Recipe title: wonton noodles\n",
      " Ingredients: ['pepper', 'pork', 'pressed', 'onions', 'candlenuts', 'water', 'potatoes', 'sauce', 'shrimp', 'rice', 'soy']\n",
      "\n",
      "Recipe title: medium egg noodles\n",
      " Ingredients: ['savoy', 'all-purpose', 'pepper', 'capsicum', 'flour', 'garlic', 'chow', 'cheese', 'oil', 'onions', 'vermicelli', 'cloves', 'raw', 'rice', 'lima', 'black', 'half', 'olive']\n",
      "\n",
      "Recipe title: thai noodles\n",
      " Ingredients: ['all-purpose', 'pepper', 'flour', 'parsley', 'garlic', 'parmesan', 'oil', 'skinless', 'cheese', 'cabbage', 'baking', 'grated', 'Sriracha', 'beef', 'fresh', 'rice', 'cream', 'chicken', 'milk', 'breasts', 'olive']\n",
      "\n",
      "Recipe title: oven-ready lasagna noodles\n",
      " Ingredients: ['pepper', 'mein', 'paste', 'garlic', 'spinach', 'cheese', 'mozzarella', 'oil', 'onions', 'ricotta', 'cabbage', 'beef', 'sauce', 'rice', 'ground', 'olive']\n",
      "\n",
      "Recipe title: chow mein noodles\n",
      " Ingredients: ['pepper', 'chopped', 'garlic', 'pressed', 'onions', 'low', 'candlenuts', 'tomato', 'water', 'low-sodium', 'sauce', 'shrimp', 'rice', 'soy', 'sodium']\n",
      "\n",
      "Recipe title: yakisoba noodles\n",
      " Ingredients: ['tamari', 'carrots', 'oil', 'onions', 'sesame', 'sauce', 'rice', 'soy', 'vinegar', 'salt', 'green']\n",
      "\n",
      "Recipe title: chinese noodles\n",
      " Ingredients: ['anise', 'chinese', 'chopped', 'sour', 'oil', 'canola', 'onions', 'ginger', 'lean', 'fresh', 'powder', 'sauce', 'rice', 'green']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for title, reply in zip(questions, replies):\n",
    "    print(\"Recipe title: {}\\n Ingredients: {}\\n\".format(title, reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
