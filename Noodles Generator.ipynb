{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import itertools\n",
    "from random import sample\n",
    "import numpy as np\n",
    "# Preprocessing\n",
    "data_file = 'train.json'\n",
    "noodles_data = 'noodles.txt'\n",
    "\n",
    "UNK = 'unk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(data_file) as recipe_data:\n",
    "    data = json.load(recipe_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ingredients_list = [item['ingredients'] for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noodles_only = []\n",
    "for ingredients in ingredients_list:\n",
    "    for ingredient in ingredients:\n",
    "        if 'noodles' in ingredient:\n",
    "            noodles_only.append(ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Writing to the file\n",
    "with open(noodles_data, 'w') as nd:\n",
    "    json.dump(noodles_only, nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pork loin', 'roasted peanuts', 'chopped cilantro fresh', 'hoisin sauce', 'creamy peanut butter', 'chopped fresh mint', 'thai basil', 'rice', 'medium shrimp', 'water', 'rice noodles', 'beansprouts']\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into recipe title and ingredients\n",
    "print(noodles_only[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def title_ingredient_split(ingredients):\n",
    "    title = {ingredient for ingredient in ingredients if 'noodles' in ingredient}\n",
    "    _ingredients = list(set(ingredients) - set(title))\n",
    "    return list(title)[0], _ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get all \n",
    "titles = []\n",
    "ingredients = []\n",
    "V = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for noodles in noodles_only:\n",
    "    title, ingredient = title_ingredient_split(noodles)\n",
    "    for ing in ingredient:\n",
    "        for i in ing.split(\" \"):\n",
    "            V.append(i)\n",
    "    for t in title.split(\" \"):\n",
    "        V.append(t)\n",
    "    titles.append(title.lower())\n",
    "    ingredients.append(\" \".join(ing for ing in ingredient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Vocabulary = list(set(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EN_WHITELIST = '0123456789abcdefghijklmnopqrstuvwxyz ' # space is included in whitelist\n",
    "EN_BLACKLIST = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\''\n",
    "\n",
    "def zero_pad(qtokenized, atokenized, w2idx):\n",
    "    # num of rows\n",
    "    data_len = len(qtokenized)\n",
    "    \n",
    "    print(data_len)\n",
    "\n",
    "    # numpy arrays to store indices\n",
    "    idx_q = np.zeros([data_len, limit['maxq']], dtype=np.int32) \n",
    "    idx_a = np.zeros([data_len, limit['maxa']], dtype=np.int32)\n",
    "\n",
    "    for i in range(data_len):\n",
    "        q_indices = pad_seq(qtokenized[i], w2idx, limit['maxq'])\n",
    "        a_indices = pad_seq(atokenized[i], w2idx, limit['maxa'])\n",
    "\n",
    "        idx_q[i] = np.array(q_indices)\n",
    "        idx_a[i] = np.array(a_indices)\n",
    "\n",
    "    return idx_q, idx_a\n",
    "\n",
    "\n",
    "def pad_seq(seq, lookup, maxlen):\n",
    "    indices = []\n",
    "    for word in seq:\n",
    "        if word in lookup:\n",
    "            indices.append(lookup[word])\n",
    "        else:\n",
    "            indices.append(lookup[UNK])\n",
    "    return indices + [0]*(maxlen - len(seq))\n",
    "\n",
    "\n",
    "def index_(tokenized_sentences, vocab_size):\n",
    "    # get frequency distribution\n",
    "    freq_dist = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "    # get vocabulary of 'vocab_size' most used words\n",
    "    vocab = freq_dist.most_common(vocab_size)\n",
    "    # index2word\n",
    "    index2word = ['_'] + [UNK] + [ x[0] for x in vocab ]\n",
    "    # word2index\n",
    "    word2index = dict([(w,i) for i,w in enumerate(index2word)] )\n",
    "    return index2word, word2index, freq_dist\n",
    "\n",
    "\n",
    "def split_dataset(x, y, ratio = [0.7, 0.15, 0.15] ):\n",
    "    # number of examples\n",
    "    data_len = len(x)\n",
    "    lens = [ int(data_len*item) for item in ratio ]\n",
    "\n",
    "    trainX, trainY = x[:lens[0]], y[:lens[0]]\n",
    "    testX, testY = x[lens[0]:lens[0]+lens[1]], y[lens[0]:lens[0]+lens[1]]\n",
    "    validX, validY = x[-lens[-1]:], y[-lens[-1]:]\n",
    "\n",
    "    return (trainX,trainY), (testX,testY), (validX,validY)\n",
    "\n",
    "\n",
    "\n",
    "def batch_gen(x, y, batch_size):\n",
    "    # infinite while\n",
    "    while True:\n",
    "        for i in range(0, len(x), batch_size):\n",
    "            if (i+1)*batch_size < len(x):\n",
    "                yield x[i : (i+1)*batch_size ].T, y[i : (i+1)*batch_size ].T\n",
    "\n",
    "\n",
    "def rand_batch_gen(x, y, batch_size):\n",
    "    while True:\n",
    "        sample_idx = sample(list(np.arange(len(x))), batch_size)\n",
    "        yield x[sample_idx].T, y[sample_idx].T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def decode(sequence, lookup, separator=''): # 0 used for padding, is ignored\n",
    "    return separator.join([ lookup[element] for element in sequence if element ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Begin processing\n",
    "recipe_tokenized = [ wordlist.split(' ') for wordlist in ingredients ]\n",
    "title_tokenized = [ wordlist.split(' ') for wordlist in titles ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1412\n"
     ]
    }
   ],
   "source": [
    "limit = {\n",
    "        'maxq' : 100,\n",
    "        'minq' : 1,\n",
    "        'maxa' : 5,\n",
    "        'mina' : 1\n",
    "        }\n",
    "\n",
    "\n",
    "idx2w, w2idx, freq_dist = index_( recipe_tokenized + title_tokenized, vocab_size=VOCAB_SIZE)\n",
    "idx_q, idx_a = zero_pad(recipe_tokenized, title_tokenized, w2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(trainX, trainY), (testX, testY), (validX, validY) = split_dataset(idx_a, idx_q, ratio=[.8, .1, .1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let us now save the necessary dictionaries\n",
    "metadata = {\n",
    "        'w2idx' : w2idx,\n",
    "        'idx2w' : idx2w,\n",
    "        'limit' : limit,\n",
    "        'freq_dist' : freq_dist\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xseq_len = trainX.shape[-1]\n",
    "yseq_len = trainY.shape[-1]\n",
    "batch_size = 4\n",
    "xvocab_size = len(metadata['idx2w'])  \n",
    "yvocab_size = xvocab_size\n",
    "emb_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seq2seq_wrapper\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'seq2seq_wrapper' from 'E:\\\\rnn\\\\practical_seq2seq\\\\seq2seq_wrapper.py'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(seq2seq_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<log> Building Graph </log>"
     ]
    }
   ],
   "source": [
    "model = seq2seq_wrapper.Seq2Seq(xseq_len=xseq_len,\n",
    "                               yseq_len=yseq_len,\n",
    "                               xvocab_size=xvocab_size,\n",
    "                               yvocab_size=yvocab_size,\n",
    "                               ckpt_path='ckpt/noodles/',\n",
    "                               emb_dim=emb_,\n",
    "                               num_layers=3,\n",
    "                                epochs=100000,\n",
    "                                lr=0.0001\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_batch_gen = rand_batch_gen(validX, validY, 141)\n",
    "test_batch_gen = rand_batch_gen(testX, testY, 141)\n",
    "train_batch_gen = rand_batch_gen(trainX, trainY, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<log> Training started </log>\n",
      "Epoch : 0\n",
      "Epoch : 1\n",
      "Epoch : 2\n",
      "Epoch : 3\n",
      "Epoch : 4\n",
      "Epoch : 5\n",
      "Epoch : 6\n",
      "Epoch : 7\n",
      "Epoch : 8\n",
      "Epoch : 9\n",
      "Epoch : 10\n",
      "Epoch : 11\n",
      "Epoch : 12\n",
      "Epoch : 13\n",
      "Epoch : 14\n",
      "Epoch : 15\n",
      "Epoch : 16\n",
      "Epoch : 17\n",
      "Epoch : 18\n",
      "Epoch : 19\n",
      "Epoch : 20\n",
      "Epoch : 21\n",
      "Epoch : 22\n",
      "Epoch : 23\n",
      "Epoch : 24\n",
      "Epoch : 25\n",
      "Epoch : 26\n",
      "Epoch : 27\n",
      "Epoch : 28\n",
      "Epoch : 29\n",
      "Epoch : 30\n",
      "Epoch : 31\n",
      "Epoch : 32\n",
      "Epoch : 33\n",
      "Epoch : 34\n",
      "Epoch : 35\n",
      "Epoch : 36\n",
      "Epoch : 37\n",
      "Epoch : 38\n",
      "Epoch : 39\n",
      "Epoch : 40\n",
      "Epoch : 41\n",
      "Epoch : 42\n",
      "Epoch : 43\n",
      "Epoch : 44\n",
      "Epoch : 45\n",
      "Epoch : 46\n",
      "Epoch : 47\n",
      "Epoch : 48\n",
      "Epoch : 49\n",
      "Epoch : 50\n",
      "Epoch : 51\n",
      "Epoch : 52\n",
      "Epoch : 53\n",
      "Epoch : 54\n",
      "Epoch : 55\n",
      "Epoch : 56\n",
      "Epoch : 57\n",
      "Epoch : 58\n",
      "Epoch : 59\n",
      "Epoch : 60\n",
      "Epoch : 61\n",
      "Epoch : 62\n",
      "Epoch : 63\n",
      "Epoch : 64\n",
      "Epoch : 65\n",
      "Epoch : 66\n",
      "Epoch : 67\n",
      "Epoch : 68\n",
      "Epoch : 69\n",
      "Epoch : 70\n",
      "Epoch : 71\n",
      "Epoch : 72\n",
      "Epoch : 73\n",
      "Epoch : 74\n",
      "Epoch : 75\n",
      "Epoch : 76\n",
      "Epoch : 77\n",
      "Epoch : 78\n",
      "Epoch : 79\n",
      "Epoch : 80\n",
      "Epoch : 81\n",
      "Epoch : 82\n",
      "Epoch : 83\n",
      "Epoch : 84\n",
      "Epoch : 85\n",
      "Epoch : 86\n",
      "Epoch : 87\n",
      "Epoch : 88\n",
      "Epoch : 89\n",
      "Epoch : 90\n",
      "Epoch : 91\n",
      "Epoch : 92\n",
      "Epoch : 93\n",
      "Epoch : 94\n",
      "Epoch : 95\n",
      "Epoch : 96\n",
      "Epoch : 97\n",
      "Epoch : 98\n",
      "Epoch : 99\n",
      "Epoch : 100\n",
      "Epoch : 101\n",
      "Epoch : 102\n",
      "Epoch : 103\n",
      "Epoch : 104\n",
      "Epoch : 105\n",
      "Epoch : 106\n",
      "Epoch : 107\n",
      "Epoch : 108\n",
      "Epoch : 109\n",
      "Epoch : 110\n",
      "Epoch : 111\n",
      "Epoch : 112\n",
      "Epoch : 113\n",
      "Epoch : 114\n",
      "Epoch : 115\n",
      "Epoch : 116\n",
      "Epoch : 117\n",
      "Epoch : 118\n",
      "Epoch : 119\n",
      "Epoch : 120\n",
      "Epoch : 121\n",
      "Epoch : 122\n",
      "Epoch : 123\n",
      "Epoch : 124\n",
      "Epoch : 125\n",
      "Epoch : 126\n",
      "Epoch : 127\n",
      "Epoch : 128\n",
      "Epoch : 129\n",
      "Epoch : 130\n",
      "Epoch : 131\n",
      "Epoch : 132\n",
      "Epoch : 133\n",
      "Epoch : 134\n",
      "Epoch : 135\n",
      "Epoch : 136\n",
      "Epoch : 137\n",
      "Epoch : 138\n",
      "Epoch : 139\n",
      "Epoch : 140\n",
      "Epoch : 141\n",
      "Epoch : 142\n",
      "Epoch : 143\n",
      "Epoch : 144\n",
      "Epoch : 145\n",
      "Epoch : 146\n",
      "Epoch : 147\n",
      "Epoch : 148\n",
      "Epoch : 149\n",
      "Epoch : 150\n",
      "Epoch : 151\n",
      "Epoch : 152\n",
      "Epoch : 153\n",
      "Epoch : 154\n",
      "Epoch : 155\n",
      "Epoch : 156\n",
      "Epoch : 157\n",
      "Epoch : 158\n",
      "Epoch : 159\n",
      "Epoch : 160\n",
      "Epoch : 161\n",
      "Epoch : 162\n",
      "Epoch : 163\n",
      "Epoch : 164\n",
      "Epoch : 165\n",
      "Epoch : 166\n",
      "Epoch : 167\n",
      "Epoch : 168\n",
      "Epoch : 169\n",
      "Epoch : 170\n",
      "Epoch : 171\n",
      "Epoch : 172\n",
      "Epoch : 173\n",
      "Epoch : 174\n",
      "Epoch : 175\n",
      "Epoch : 176\n",
      "Epoch : 177\n",
      "Epoch : 178\n",
      "Epoch : 179\n",
      "Epoch : 180\n",
      "Epoch : 181\n",
      "Epoch : 182\n",
      "Epoch : 183\n",
      "Epoch : 184\n",
      "Epoch : 185\n",
      "Epoch : 186\n",
      "Epoch : 187\n",
      "Epoch : 188\n",
      "Epoch : 189\n",
      "Epoch : 190\n",
      "Epoch : 191\n",
      "Epoch : 192\n",
      "Epoch : 193\n",
      "Epoch : 194\n",
      "Epoch : 195\n",
      "Epoch : 196\n",
      "Epoch : 197\n",
      "Epoch : 198\n",
      "Epoch : 199\n",
      "Epoch : 200\n",
      "Epoch : 201\n",
      "Epoch : 202\n",
      "Epoch : 203\n",
      "Epoch : 204\n",
      "Epoch : 205\n",
      "Epoch : 206\n",
      "Epoch : 207\n",
      "Epoch : 208\n",
      "Epoch : 209\n",
      "Epoch : 210\n",
      "Epoch : 211\n",
      "Epoch : 212\n",
      "Epoch : 213\n",
      "Epoch : 214\n",
      "Epoch : 215\n",
      "Epoch : 216\n",
      "Epoch : 217\n",
      "Epoch : 218\n",
      "Epoch : 219\n",
      "Epoch : 220\n",
      "Epoch : 221\n",
      "Epoch : 222\n",
      "Epoch : 223\n",
      "Epoch : 224\n",
      "Epoch : 225\n",
      "Epoch : 226\n",
      "Epoch : 227\n",
      "Epoch : 228\n",
      "Epoch : 229\n",
      "Epoch : 230\n",
      "Epoch : 231\n",
      "Epoch : 232\n",
      "Epoch : 233\n",
      "Epoch : 234\n",
      "Epoch : 235\n",
      "Epoch : 236\n",
      "Epoch : 237\n",
      "Epoch : 238\n",
      "Epoch : 239\n",
      "Epoch : 240\n",
      "Epoch : 241\n",
      "Epoch : 242\n",
      "Epoch : 243\n",
      "Epoch : 244\n",
      "Epoch : 245\n",
      "Epoch : 246\n",
      "Epoch : 247\n",
      "Epoch : 248\n",
      "Epoch : 249\n",
      "Epoch : 250\n",
      "Epoch : 251\n",
      "Epoch : 252\n",
      "Epoch : 253\n",
      "Epoch : 254\n",
      "Epoch : 255\n",
      "Epoch : 256\n",
      "Epoch : 257\n",
      "Epoch : 258\n",
      "Epoch : 259\n",
      "Epoch : 260\n",
      "Epoch : 261\n",
      "Epoch : 262\n",
      "Epoch : 263\n",
      "Epoch : 264\n",
      "Epoch : 265\n",
      "Epoch : 266\n",
      "Epoch : 267\n",
      "Epoch : 268\n",
      "Epoch : 269\n",
      "Epoch : 270\n",
      "Epoch : 271\n",
      "Epoch : 272\n",
      "Epoch : 273\n",
      "Epoch : 274\n",
      "Epoch : 275\n",
      "Epoch : 276\n",
      "Epoch : 277\n",
      "Epoch : 278\n",
      "Epoch : 279\n",
      "Epoch : 280\n",
      "Epoch : 281\n",
      "Epoch : 282\n",
      "Epoch : 283\n",
      "Epoch : 284\n",
      "Epoch : 285\n",
      "Epoch : 286\n",
      "Epoch : 287\n",
      "Epoch : 288\n",
      "Epoch : 289\n",
      "Epoch : 290\n",
      "Epoch : 291\n",
      "Epoch : 292\n",
      "Epoch : 293\n",
      "Epoch : 294\n",
      "Epoch : 295\n",
      "Epoch : 296\n",
      "Epoch : 297\n",
      "Epoch : 298\n",
      "Epoch : 299\n",
      "\n",
      "Model saved to disk at iteration #299\n",
      "val   loss : 1.415226\n",
      "Epoch : 300\n",
      "Epoch : 301\n",
      "Epoch : 302\n",
      "Epoch : 303\n",
      "Epoch : 304\n",
      "Epoch : 305\n",
      "Epoch : 306\n",
      "Epoch : 307\n",
      "Epoch : 308\n",
      "Epoch : 309\n",
      "Epoch : 310\n",
      "Epoch : 311\n",
      "Epoch : 312\n",
      "Epoch : 313\n",
      "Epoch : 314\n",
      "Epoch : 315\n",
      "Epoch : 316\n",
      "Epoch : 317\n",
      "Epoch : 318\n",
      "Epoch : 319\n",
      "Epoch : 320\n",
      "Epoch : 321\n",
      "Epoch : 322\n",
      "Epoch : 323\n",
      "Epoch : 324\n",
      "Epoch : 325\n",
      "Epoch : 326\n",
      "Epoch : 327\n",
      "Epoch : 328\n",
      "Epoch : 329\n",
      "Epoch : 330\n",
      "Epoch : 331\n",
      "Epoch : 332\n",
      "Epoch : 333\n",
      "Epoch : 334\n",
      "Epoch : 335\n",
      "Epoch : 336\n",
      "Epoch : 337\n",
      "Epoch : 338\n",
      "Epoch : 339\n",
      "Epoch : 340\n",
      "Epoch : 341\n",
      "Epoch : 342\n",
      "Epoch : 343\n",
      "Epoch : 344\n",
      "Epoch : 345\n",
      "Epoch : 346\n",
      "Epoch : 347\n",
      "Epoch : 348\n",
      "Epoch : 349\n",
      "Epoch : 350\n",
      "Epoch : 351\n",
      "Epoch : 352\n",
      "Epoch : 353\n",
      "Epoch : 354\n",
      "Epoch : 355\n",
      "Epoch : 356\n",
      "Epoch : 357\n",
      "Epoch : 358\n",
      "Epoch : 359\n",
      "Epoch : 360\n",
      "Epoch : 361\n",
      "Epoch : 362\n",
      "Epoch : 363\n",
      "Epoch : 364\n",
      "Epoch : 365\n",
      "Epoch : 366\n",
      "Epoch : 367\n",
      "Epoch : 368\n",
      "Epoch : 369\n",
      "Epoch : 370\n",
      "Epoch : 371\n",
      "Epoch : 372\n",
      "Epoch : 373\n",
      "Epoch : 374\n",
      "Epoch : 375\n",
      "Epoch : 376\n",
      "Epoch : 377\n",
      "Epoch : 378\n",
      "Epoch : 379\n",
      "Epoch : 380\n",
      "Epoch : 381\n",
      "Epoch : 382\n",
      "Epoch : 383\n",
      "Epoch : 384\n",
      "Epoch : 385\n",
      "Epoch : 386\n",
      "Epoch : 387\n",
      "Epoch : 388\n",
      "Epoch : 389\n",
      "Epoch : 390\n",
      "Epoch : 391\n",
      "Epoch : 392\n",
      "Epoch : 393\n",
      "Epoch : 394\n",
      "Epoch : 395\n",
      "Epoch : 396\n",
      "Epoch : 397\n",
      "Epoch : 398\n",
      "Epoch : 399\n",
      "Epoch : 400\n",
      "Epoch : 401\n",
      "Epoch : 402\n",
      "Epoch : 403\n",
      "Epoch : 404\n",
      "Epoch : 405\n",
      "Epoch : 406\n",
      "Epoch : 407\n",
      "Epoch : 408\n",
      "Epoch : 409\n",
      "Epoch : 410\n",
      "Epoch : 411\n",
      "Epoch : 412\n",
      "Epoch : 413\n",
      "Epoch : 414\n",
      "Epoch : 415\n",
      "Epoch : 416\n",
      "Epoch : 417\n",
      "Epoch : 418\n",
      "Epoch : 419\n",
      "Epoch : 420\n",
      "Epoch : 421\n",
      "Epoch : 422\n",
      "Epoch : 423\n",
      "Epoch : 424\n",
      "Epoch : 425\n",
      "Epoch : 426\n",
      "Epoch : 427\n",
      "Epoch : 428\n",
      "Epoch : 429\n",
      "Epoch : 430\n",
      "Epoch : 431\n",
      "Epoch : 432\n",
      "Epoch : 433\n",
      "Epoch : 434\n",
      "Epoch : 435\n",
      "Epoch : 436\n",
      "Epoch : 437\n",
      "Epoch : 438\n",
      "Epoch : 439\n",
      "Epoch : 440\n",
      "Epoch : 441\n",
      "Epoch : 442\n",
      "Epoch : 443\n",
      "Epoch : 444\n",
      "Epoch : 445\n",
      "Epoch : 446\n",
      "Epoch : 447\n",
      "Epoch : 448\n",
      "Epoch : 449\n",
      "Epoch : 450\n",
      "Epoch : 451\n",
      "Epoch : 452\n",
      "Epoch : 453\n",
      "Epoch : 454\n",
      "Epoch : 455\n",
      "Epoch : 456\n",
      "Epoch : 457\n",
      "Epoch : 458\n",
      "Epoch : 459\n",
      "Epoch : 460\n",
      "Epoch : 461\n",
      "Epoch : 462\n",
      "Epoch : 463\n",
      "Epoch : 464\n",
      "Epoch : 465\n",
      "Epoch : 466\n",
      "Epoch : 467\n",
      "Epoch : 468\n",
      "Epoch : 469\n",
      "Interrupted by user at iteration 469\n"
     ]
    }
   ],
   "source": [
    "sess = model.train(train_batch_gen, val_batch_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141, 100)\n"
     ]
    }
   ],
   "source": [
    "input_ = test_batch_gen.__next__()[0]\n",
    "output = model.predict(sess, input_)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title : [rice noodles]; recipe : [lime oil oil oil oil oil oil sauce sauce sauce sauce sauce]\n",
      "title : [rice stick noodles]; recipe : [lime oil oil oil oil oil oil sauce sauce sauce sauce sauce sauce]\n",
      "title : [noodles]; recipe : [lime salt oil oil oil oil oil sauce sauce sauce sauce sauce]\n",
      "title : [egg noodles]; recipe : [lime oil oil oil oil oil sauce sauce sauce sauce sauce sauce]\n",
      "title : [soba noodles]; recipe : [lime oil oil oil oil oil sauce sauce sauce sauce sauce]\n",
      "title : [chuka soba noodles]; recipe : [salt oil oil oil oil sauce sauce sauce sauce sauce]\n",
      "title : [mung bean noodles]; recipe : [salt oil oil oil oil sauce sauce sauce sauce sauce sauce]\n",
      "title : [medium egg noodles]; recipe : [lime oil oil oil oil sauce sauce sauce sauce sauce sauce]\n"
     ]
    }
   ],
   "source": [
    "replies = []\n",
    "for ii, oi in zip(input_.T, output):\n",
    "    q = decode(sequence=ii, lookup=metadata['idx2w'], separator=' ')\n",
    "    decoded = decode(sequence=oi, lookup=metadata['idx2w'], separator=' ').split(' ')\n",
    "    if decoded.count('unk') == 0:\n",
    "        if decoded not in replies:\n",
    "            print('title : [{0}]; recipe : [{1}]'.format(q, (' '.join(decoded))))\n",
    "            replies.append(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(output[0], lookup=metadata['idx2w'], separator=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vegetable'"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(train_batch_gen.__next__()[0][30], lookup=metadata['idx2w'], separator=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([211, 313,  11,  23,  76,  11,  11,  16, 103,  16, 528,  20, 313,\n",
       "        11,  12, 225,  23, 465,  16,  11,  11,  44,  11, 528, 149,  11,\n",
       "        10,  34,  21, 117,  15,  11,  50, 106,  17, 383,  17,  10,  11,\n",
       "        10, 126,  12,  11,  32,  21,  69,  60,  97, 106,  56,  11,  31,\n",
       "        11, 215, 140,  11,  15,  13, 264, 598,  11,  12,  44,  31,  97,\n",
       "       126,  24,  44,  12,  12,  69,  16,  94,  44,  17,  49, 981,  17,\n",
       "        11,  10,  44, 349,  54, 720,  44,  32, 149,  11,  97,  31,  44,\n",
       "        11,  69, 119,  15,  31,  94, 106,  54, 143,  12,  23,  21,  97,\n",
       "       116,  34,  69,  54, 469,   8,  54,  12,  44, 282, 282,  12, 103,\n",
       "       264, 140, 964,  11,  16,  11, 482,  75,  17,  57,  60, 105,  11,\n",
       "        44,  16,  21,  44,  24, 295, 319,  31,  69,   9, 295])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
